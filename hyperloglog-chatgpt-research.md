https://chatgpt.com/share/680f10af-d5c8-8007-b7eb-2f565182eac0

Perfect, thanks for confirming. I will prepare a detailed exploration of how HyperLogLog is used for cardinality estimation, including its specific application within GreptimeDB's Flow Engine, as well as broader real-world examples in Druid, Redis, and ClickHouse. I will reference academic papers, technical blog posts, and GitHub repositories where available. I'll get started and update you once the full report is ready.

# HyperLogLog-based Cardinality Estimation

([HyperLogLog Algorithm in Pictures - Algorithms for Big Data](https://stefansavev.com/blog/an-intuitive-explanation-of-hyperloglog-algorithm-for-approximate-distinct-count/))_Figure: Conceptual illustration of HyperLogLog’s random “prefix tree” of hashed values. HLL hashes each input and records the longest run of leading zeros in each bucket to estimate distinct counts ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) )._ HyperLogLog (HLL) is a probabilistic algorithm for estimating the number of distinct elements in a dataset ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ). It hashes each item to a (large) bit string and tracks the maximum number of leading zeros seen among hash values. To reduce variance, the hash space is divided into _m_ = 2^p registers (buckets) based on the first p bits of the hash, and in each register the algorithm records the maximum “rho” (position of the first 1-bit) among elements hashing there. The overall distinct count is then computed from the harmonic mean of 2^rho over all registers (with bias-correction). The original HLL algorithm (Flajolet et al. 2007) achieves this in fixed, small memory (O(m) bytes), trading a small error for space ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ). Google’s subsequent **HLL++** variant introduced additional corrections and sparse representations to improve accuracy and reduce memory in practice. In short, HLL++ (used by systems below) further lowers error (e.g. to ~0.81% at 12KB for 2^14 registers) ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ) ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)).

- **Core Steps of HLL:** Hash each element to 64 bits. Use the first _p_ bits to select one of 2^p registers; in that register, record the maximum number of leading zeros in the remaining bits. Finally, estimate distinct count via a harmonic-mean formula over the registers.
    
- **HLL++ Improvements:** The Google HLL++ paper describes bias corrections and an “altitude” set to improve small-cardinality accuracy, as well as sparse storage of registers for low-density cases. These optimizations are broadly implemented in modern systems.
    

## GreptimeDB (Flow Engine)

GreptimeDB recently added HyperLogLog support to its **Flow Engine** (continuous aggregation query engine). As of v0.14, Greptime provides built-in aggregate functions `hll`, `hll_merge`, and `hll_count` to support approximate distinct counts in streaming SQL queries ([HyperLogLog Function Now Live, PromQL Fully Upgraded | Greptime Biweekly Report | Greptime](https://www.greptime.com/blogs/2025-03-05-greptimedb-biweekly-report.html#:~:text=db,for%20approximate%20counting)). For example, one can define a flow that groups web-log events by URL and 10-second time windows, computing an HLL state of the `user_id` column:

```sql
INSERT INTO access_log_10s
SELECT 
  url,
  date_bin('10s'::INTERVAL, ts) AS time_window,
  hll(user_id) AS state 
FROM access_log
GROUP BY url, time_window;
```

This stores a binary HLL state per URL per window ([HyperLogLog Function Now Live, PromQL Fully Upgraded | Greptime Biweekly Report | Greptime](https://www.greptime.com/blogs/2025-03-05-greptimedb-biweekly-report.html#:~:text=,AS%20state%20FROM%20access_log)). A subsequent query can then apply `hll_count(state)` to get an approximate unique user count per bucket ([HyperLogLog Function Now Live, PromQL Fully Upgraded | Greptime Biweekly Report | Greptime](https://www.greptime.com/blogs/2025-03-05-greptimedb-biweekly-report.html#:~:text=,state%29%20AS%20approx_count%20FROM%20access_log_10s)). Internally, Greptime’s HLL functions use the Rust `hyperloglogplus` library (HLL++, a Flajolet/Google-based implementation) with a fixed **precision of 14 bits** (2^14 registers) ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=const%20DEFAULT_PRECISION%3A%20u8%20%3D%2014%3B)) ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=impl%20HllState%20,)). This 2^14 configuration (≈12 KB state) yields an error on the order of ~0.8% (similar to Redis’s 0.81% at 12KB) ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)) ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=const%20DEFAULT_PRECISION%3A%20u8%20%3D%2014%3B)). Currently this precision is hard-coded (DEFAULT_PRECISION=14) and not user-tunable via SQL ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=const%20DEFAULT_PRECISION%3A%20u8%20%3D%2014%3B)). As a Flow Engine feature, HLL is integrated into Greptime’s continuous query pipeline and can be combined with time windows and other aggregates (e.g. `sum`, `count`) to maintain rolling distinct counts over sliding intervals.

- **Implementation:** Provided as SQL UDAFs: `hll(col)` builds an HLL state from a string column, `hll_merge(state)` merges binary HLL states, and `hll_count(state)` returns the final count ([HyperLogLog Function Now Live, PromQL Fully Upgraded | Greptime Biweekly Report | Greptime](https://www.greptime.com/blogs/2025-03-05-greptimedb-biweekly-report.html#:~:text=db,for%20approximate%20counting)) ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=const%20DEFAULT_PRECISION%3A%20u8%20%3D%2014%3B)). These functions are registered in Greptime’s query engine (DataFusion) and can run in both batch and streaming modes of the Flow Engine.
    
- **Tuning:** Greptime uses HyperLogLog++ with 2^14 registers by default (precision 14). This fixed setup uses ≈12KB per state and yields ~0.8% error; there is currently no SQL option to change the precision, though the underlying code (DEFAULT_PRECISION=14) is visible in source ([feat: impl `hll_state`, `hll_merge` and `hll_calc` for incremental distinct counting by waynexia · Pull Request #5579 · GreptimeTeam/greptimedb · GitHub](https://github.com/GreptimeTeam/greptimedb/pull/5579/files#:~:text=const%20DEFAULT_PRECISION%3A%20u8%20%3D%2014%3B)).
    
- **Use Cases:** Greptime envisions HLL for high-cardinality metrics/traffic. For example, counting distinct user sessions per time window in real time, or unique error IDs in log streams. By using HLL in a continuous aggregation (Flow), Greptime avoids storing full sets of IDs, greatly reducing memory overhead for stateful queries on large logs or metrics.
    

## Apache Druid

Druid has long supported HLL-based distinct counting through its **cardinality** and **hyperUnique** aggregators, and more recently via the DataSketches extension. The built-in _hyperUnique_ aggregator works at ingestion/query time on a single dimension: it accepts a dimension value (or a pre-aggregated Base64 HLL blob) and maintains an HLL state per row ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Uses%20HyperLogLog%20to%20compute%20the,metric%20at%20indexing%20time)). Queries can then compute the unique count of that dimension using the HLL state. There is also a generic _cardinality_ aggregator that can work over one or multiple dimensions at query time, using HyperLogLog internally ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Computes%20the%20cardinality%20of%20a,individual%20values%20of%20a%20dimension)). Compared to exact distinct, both are fast but return only approximate counts (optionally rounded via a `round` flag) ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=The%20HyperLogLog%20algorithm%20generates%20decimal,time)) ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=,time)). Druid’s SQL layer by default maps `COUNT(DISTINCT x)` to these HLL-based aggregators (or the newer DataSketches HLL/Theta if configured) ([SQL aggregation functions | Apache® Druid](https://druid.apache.org/docs/latest/querying/sql-aggregations/#:~:text=Counts%20distinct%20values%20of%20,useApproximateCountDistinct)). In fact, Druid documentation notes that `APPROX_COUNT_DISTINCT(expr)` uses either the built-in hyperUnique/cardinality (HLL variant) or the DataSketches HLL, depending on configuration ([SQL aggregation functions | Apache® Druid](https://druid.apache.org/docs/latest/querying/sql-aggregations/#:~:text=Counts%20distinct%20values%20of%20,useApproximateCountDistinct)) ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Computes%20the%20cardinality%20of%20a,individual%20values%20of%20a%20dimension)).

- **Aggregators:**
    
    - _HyperUnique aggregator_ – used in ingestion specs. Takes a `fieldName` of a dimension and aggregates HLL sketches at index time; produces an estimated count at query time ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Uses%20HyperLogLog%20to%20compute%20the,metric%20at%20indexing%20time)). It can also ingest pre-computed HLL states (with `isInputHyperUnique=true`) ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=,time)).
        
    - _Cardinality aggregator_ – a query-time HLL over one or more dimensions. It computes `COUNT(DISTINCT)` across rows and columns using HyperLogLog ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Computes%20the%20cardinality%20of%20a,individual%20values%20of%20a%20dimension)). It supports options like `byRow` (distinct combinations vs. values) and `round` to round estimates. The docs caution this can be slower than pre-aggregating via hyperUnique ([Cardinality/HyperUnique aggregators | Apache® Druid](https://druid.apache.org/docs/latest/querying/hll-old/#:~:text=Computes%20the%20cardinality%20of%20a,individual%20values%20of%20a%20dimension)).
        
    - _DataSketches HLL (extension)_ – an alternative HLL implementation from Apache DataSketches. When the `druid-datasketches` extension is enabled, Druid provides `HLLSketchBuild` (for ingestion) and `HLLSketchMerge` (for query) aggregators ([DataSketches HLL Sketch module | Apache® Druid](https://druid.apache.org/docs/latest/development/extensions-core/datasketches-hll#:~:text=Aggregators)). These allow tuning `lgK` (log2 of bucket count, default 12) and `tgtHllType` for space/accuracy ([DataSketches HLL Sketch module | Apache® Druid](https://druid.apache.org/docs/latest/development/extensions-core/datasketches-hll#:~:text=,control%20whether%20all%20aggregators%20are)). The SQL parameter `druid.sql.approxCountDistinct.function` can be set to use `APPROX_COUNT_DISTINCT_DS_HLL` (DataSketches HLL) instead of the built-in HLL ([SQL aggregation functions | Apache® Druid](https://druid.apache.org/docs/latest/querying/sql-aggregations/#:~:text=Counts%20distinct%20values%20of%20,useApproximateCountDistinct)).
        
- **Tuning & Variants:** DataSketches HLL offers more flexibility. For example, by setting `lgK=17` (default in `uniqCombined` below) one can use 2^17 buckets (~131K) with 6-bit registers (~96KB per sketch) ([uniqCombined | ClickHouse Docs](https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/uniqcombined#:~:text=%2A%20%60HLL_precision%60%3A%20The%20base,or%20numeric%20types)). Druid’s built-in HLL is less tunable (it internally picks a precision, and exposes only “round”).
    
- **Use Cases:** Druid’s HLL features are widely used in real-time analytics and OLAP workloads. Typical examples include counting unique visitors per interval, distinct ad impressions, or unique source IPs in logs. For instance, a Druid data source might include an ingestion-time hyperUnique metric for `user_id`, allowing fast SQL queries like `SELECT country, hyperUnique(user_id) FROM table GROUP BY country` to estimate unique users by country. The _cardinality_ aggregator can be used when one needs the distinct of a multi-column set.
    

## Redis

Redis implements HyperLogLog as a native probabilistic data structure with the commands **PFADD**, **PFCOUNT**, and **PFMERGE**. Redis’s HLL is always exactly 12 kilobytes per key (with up to ~16,384 registers) and provides about **0.81% standard error** ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)), consistent with using 2^14 buckets. In fact, the original Redis announcement notes these parameters and cites both Flajolet’s paper and Google’s HLL++ improvements ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ). Internally Redis uses the HLL++ algorithm (leveraging the Google optimizations) to pack registers efficiently. The usage is simple: each `PFADD key element` adds an element into the HLL (hashing it internally), and `PFCOUNT key` returns the approximate cardinality. Multiple HLLs can be merged with `PFMERGE dest source1 source2 ...`. Since the HLL in Redis is encoded as a binary string, users can even GET/SET the raw HLL state if needed.

- **Parameters:** Fixed precision = 14 bits (2^14 registers), 6 bits per register (in Redis’s internal encoding), for a maximum of 12KB memory. This setup yields ~0.81% error ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)). Redis does not expose any configuration for HLL size – the 12KB size is constant and chosen to cover very large cardinalities (up to 2^64).
    
- **Implementation:** The Redis HLL commands are written in C as part of the core; they were introduced by antirez (Salvatore Sanfilippo) in 2013 ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ). As the docs state: “The Redis HyperLogLog implementation uses up to 12 KB and provides a standard error of 0.81%” ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)). This implementation is essentially Google’s HLL++ variant (the reference [50] and [55] are explicitly cited in the blog).
    
- **Use Cases:** Redis’s HLL is commonly used for web analytics and event monitoring where low-memory, approximate distinct counts are needed. For example, tracking the number of unique visitors (e.g. unique IP addresses or user IDs) in a day, counting distinct search queries, or distinct sessions. Many companies use Redis HLLs for metrics that would be infeasible to store explicitly (e.g. counting millions of unique events per day). Because PFCOUNT is O(1) in storage and time, it scales to very large datasets with minimal performance impact.
    

## ClickHouse

ClickHouse supports HLL-based distinct aggregation through special “uniq” functions. The basic one is `uniqHLL12(x)`, which applies HyperLogLog with 2^12 (4096) buckets of 5 bits each (about 2.5 KB state) ([uniqHLL12 | ClickHouse Docs](https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqhll12#:~:text=2%5E12%205,1B%2B%20elements)). As documented, `uniqHLL12` is very lightweight but not very precise for small cardinalities: it can have up to ~10% error when fewer than 10K distinct values, improving to ~1.6% error for 10K–100M values ([uniqHLL12 | ClickHouse Docs](https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqhll12#:~:text=2%5E12%205,1B%2B%20elements)). Because of this, ClickHouse generally recommends the hybrid `uniqCombined` or `uniqCombined64` instead.

`uniqCombined(col1, col2, ...)` uses multiple strategies: it starts with an array or hash for low cardinality, then switches to HyperLogLog once the set is large ([uniqCombined | ClickHouse Docs](https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/uniqcombined#:~:text=,on%20the%20query%20processing%20order)). It also supports an optional `HLL_precision` parameter (the base-2 log of bucket count). By default, `uniqCombined` uses `HLL_precision=17`, i.e. 2^17 ≈ 131K buckets (6 bits each, ~96KB) ([uniqCombined | ClickHouse Docs](https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/uniqcombined#:~:text=%2A%20%60HLL_precision%60%3A%20The%20base,or%20numeric%20types)). This larger HLL yields much lower error than `uniqHLL12`. The function guarantees deterministic results (order-independent). In practice, one would use `uniqCombined` for most analytics queries, reserving `uniqExact` only for very small or critical counts.

- **Functions:**
    
    - **`uniqHLL12(expr)`** – A pure HyperLogLog with 2^12 registers. Very fast and low-memory (~2.5KB per state) but only recommended when occasional 5–10% error is acceptable ([uniqHLL12 | ClickHouse Docs](https://clickhouse.com/docs/en/sql-reference/aggregate-functions/reference/uniqhll12#:~:text=2%5E12%205,1B%2B%20elements)).
        
    - **`uniqCombined([HLL_precision,] expr…)`** – The recommended approximate function. Uses 32-bit or 64-bit hashes and switches algorithms; by default it uses HLL with 2^17 registers (~96KB) for large groups ([uniqCombined | ClickHouse Docs](https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/uniqcombined#:~:text=%2A%20%60HLL_precision%60%3A%20The%20base,or%20numeric%20types)) ([uniqCombined | ClickHouse Docs](https://clickhouse.com/docs/sql-reference/aggregate-functions/reference/uniqcombined#:~:text=,on%20the%20query%20processing%20order)). The user can pass a lower `HLL_precision` (e.g. 16 or 15) to trade accuracy for space.
        
    - **`uniqExact(expr)`** – Exact distinct (full hash table), used automatically by ClickHouse for small sets.
        
    - (ClickHouse also offers `uniqTheta` which uses an Apache DataSketches Theta sketch, and other specialized versions, but these are beyond HLL scope.)
        
- **Tuning:** The `HLL_precision` parameter in `uniqCombined` (and `uniqCombined64`) controls memory vs. error. Precision=17 is default; dropping to 14 or 15 reduces state size by 4–8× at the cost of higher error. There is also a `uniqCombined64` variant that uses 64-bit hashes for inputs (for >2^32 distinct values).
    
- **Use Cases:** In ClickHouse’s OLAP workloads (web analytics, telemetry, logs), these functions allow fast, distributed computation of distinct counts. For example, to count unique sessions in a large time series or logs table:
    
    ```sql
    SELECT uniqCombined(user_id) AS approx_users 
    FROM events 
    WHERE date >= '2025-01-01' AND date < '2025-02-01';
    ```
    
    This returns an estimated distinct count using HLL behind the scenes. ClickHouse’s ability to combine partial HLL states from different shards makes such large-scale distinct queries efficient.
    

**References:** The original HyperLogLog algorithm and its theoretical basis are described by Flajolet et al. ( [Redis new data structure: the HyperLogLog -](https://antirez.com/news/75#:~:text=HyperLogLog%20is%20remarkable%20as%20it,which%20seems%20quite%20unlikely) ). Google’s HLL++ improvements (widely adopted by Redis, Greptime, etc.) are detailed by Heule et al. (2013). Each database’s own docs and source (cited above) explain their HyperLogLog integration and parameters. Common real-world uses include tracking unique users, session IDs, IPs, or event types in streaming and OLAP systems ([HyperLogLog | Docs](https://redis.io/docs/latest/develop/data-types/probabilistic/hyperloglogs/#:~:text=The%20Redis%20HyperLogLog%20implementation%20uses,81)). Each system trades some precision for huge space/time savings, making cardinality queries scalable in massive data environments.
