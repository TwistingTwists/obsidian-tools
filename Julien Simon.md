Decoder only inference - working.

What is difference between encoder - decoder and decoder only models ? When to use which one ?? 

## Prefill phase 
- compute intensive, compute KV values 
- highly parallel process 

## Decode 
- one token at a time - auto regressive nature
- sequential process 

## optimisations 

- KV cache -- grows linearly with input length 
- cache size (fp16) = 2 x 2 x batch_size x seq_length x num_layers x embeddings_length 
- 2 = bytes 
- 2 = k and v are two matrices 
So, we have different types of attention so that we can shrink KV cache size and increase the batch size. 

## continuous batching 

Traditional batching 
- get 10 requests and process them at once. 
	- input and output length can vary a loooooot 
	- so some requests finish early, some late 
So do continuous batching 

- token generation (decoding?) must pause regularly to evict completed requests and Prefill new ones ??? 
- waiting_served_ratio - how often should we pause generation to do Prefill 

https://www.anyscale.com/blog/continuous-batching-llm-inference



## speculative decoding 

https://huggingface.co/blog/assisted-generation
https://arxiv.org/pdf/2211.17192

- smaller model to predict -- multi token completion in parallel 
- looking ahead with smaller model 
- ask large model to evaluate the options generated by smaller model

![[Pasted image 20251201211248.jpg]]


### speculative decoding - medusa 
- Multiple decoding heads - and select the one which works the best
