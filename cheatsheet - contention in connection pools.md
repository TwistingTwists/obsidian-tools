basic contention: min_idle + max_connections MUST be respected at all times. 
important: never shoot beyond max_connections. NEVER. 

üëâ **Net effect:** fast when idle, stable under bursts, safe under concurrency.




# short summary of Approval system in the connection pools 

## Capacity Reservation & `ApprovalIter` ‚Äî Recall Sheet

## 1Ô∏è‚É£ The core problem (first principles)

> **I need to do slow, async, fallible work without ever violating a hard capacity limit.**

In this pool:

- Capacity = `num_conns + pending_conns ‚â§ max_size`
    
- Work = creating connections (async, retrying, may fail)
    
- Many triggers:
    
    - `get()` demand
        
    - min-idle warmup
        
    - broken/expired connections
        
    - reaper
        

**Key difficulty:**  
You must decide _how many_ to start **under a lock**, but _execute_ the work **outside the lock**.

---

## 2Ô∏è‚É£ The core idea: Capacity Reservation Interface

> **Reserve capacity first. Commit or roll back later.**

### Two phases (2PC-like, single process):

1. **Reserve (prepare)**
    
    - Under lock
        
    - Increment `pending_conns`
        
    - Mint a token (`Approval`)
        
2. **Resolve (commit / rollback)**
    
    - Outside lock
        
    - Success ‚Üí convert pending ‚Üí real connection
        
    - Failure ‚Üí release reservation
        

üí° This is _not_ distributed 2PC:

- single authority
    
- no durability
    
- no blocking participants
    
- just invariant protection
    

---

## 3Ô∏è‚É£ What an `Approval` is (precise meaning)

> **An `Approval` = ‚Äúyou are allowed to attempt exactly ONE new connection.‚Äù**

- It represents _reserved pool capacity_
    
- It must be:
    
    - **consumed** on success (`put(conn, Some(approval))`)
        
    - **returned** on failure (`connect_failed(approval)`)
        

Think of it as a **connect-job ticket**, not a usage permit.

---

## 4Ô∏è‚É£ Why `ApprovalIter` exists

### What it does

- Hands out **N approvals** decided under the lock
    
- No allocation
    
- Cheap batch admission
    

```rust
let approvals = locked.approvals(config, n);
for approval in approvals {
    spawn_connect_task(approval);
}
```

### Why not `usize`

- Counts are easy to misuse
    
- Tokens force correct success/failure reconciliation
    
- Types encode correctness
    

### Why not `Vec<Approval>`

- Same semantics
    
- Unnecessary allocation
    
- Iterator is strictly better
    

---

## 5Ô∏è‚É£ Why not just a semaphore?

### Semaphores are great when:

- Capacity = the only constraint
    
- You want callers to wait for permits
    
- One permit = one unit of work
    

### This pool needs more:

- `min_idle` replenishment
    
- Separate **pending vs existing** connections
    
- Demand-based scaling (`in_flight > pending`)
    
- Batch decisions under the same lock as idle queue
    

Semaphores cap _usage_.  
Approvals reserve _creation capacity_.

---

## 6Ô∏è‚É£ Two replenishment modes (important!)

### 1. **Bulk (baseline health)**

```rust
wanted() ‚Üí ApprovalIter
```

- Ensures `min_idle`
    
- Issues many approvals at once
    
- Used on:
    
    - startup
        
    - reaping
        
    - dropped/broken connections
        

### 2. **Trickle (demand-driven)**

```rust
if in_flight > pending_conns {
    approvals = 1;
}
```

- Avoids thundering herds
    
- Reacts to contention
    
- Used in `get()`
    

üí° This split is a _key design insight_.

---

## 7Ô∏è‚É£ Approval lifecycle (memorize this flow)

```
[approved] ‚Üí pending_conns += 1
     ‚Üì
(connect async, retries)
     ‚Üì
SUCCESS ‚Üí pending--, num_conns++, push idle
FAILURE ‚Üí pending--
```

Everything funnels through this path.

---

## 8Ô∏è‚É£ Where this pattern appears in real systems

Use this pattern when:

- Capacity must **never** be exceeded
    
- Work is **slow / async / fallible**
    
- Decisions must be **atomic**, execution cannot be
    

### Common examples

- DB / Redis / HTTP connection pools
    
- Worker schedulers
    
- Bounded producer pipelines
    
- Memory / buffer pools
    
- Rate limiters with rollback
    
- Cluster schedulers (K8s-style reservations)
    

---

## 9Ô∏è‚É£ Mental shortcuts (for recall)

- **Approval ‚âà permit for creation**
    
- **ApprovalIter ‚âà batch permits**
    
- **pending_conns = prepared**
    
- **put(Some(approval)) = commit**
    
- **connect_failed = rollback**
    
- **Semaphore caps usage**
    
- **Approval caps growth**
    

---

## üîü One-line takeaway

> **`ApprovalIter` is a cheap, explicit way to reserve pool capacity upfront, so async connection creation can happen safely without ever violating invariants.**

If you remember _only one thing_, remember that.

---



# Avoiding thundering herd in the connectoin pools 

[[thundering-herd python script]]

## What ‚Äú`in_flight > pending`‚Äù actually means

- **`in_flight`**  
    = number of _active `get()` attempts_ (current demand)
    
- **`pending`**  
    = number of _connections already being created_ (supply in progress)
    

So:

> `in_flight > pending`  
> ‚áí _demand is growing faster than supply_

That is the **signal**.

---

## Why add **only 1** connection

When that signal is true, the pool does this:

```rust
approvals = 1;
```

Not N. Not ‚Äúfill to max‚Äù.

**Reason:** connection creation is slow, expensive, and failure-prone.

Adding only 1 means:

- demand increase ‚Üí _small_ supply increase
    
- repeat signals ‚Üí gradual ramp-up
    
- no sudden connection storms
    

---

## How this prevents a thundering herd

### Without trickle (bad case)

- 50 tasks call `get()` simultaneously
    
- All see ‚Äúno idle connections‚Äù
    
- All decide ‚Äúwe should create connections‚Äù
    
- 50 connection attempts start at once
    
- DB / service collapses
    

### With trickle

- First `get()` sees `in_flight=1, pending=0` ‚Üí start 1 connect
    
- Second sees `in_flight=2, pending=1` ‚Üí start 1 more
    
- Third sees `in_flight=3, pending=2` ‚Üí start 1 more
    
- ‚Ä¶
    
- Growth is **linear and controlled**
    

Each extra connection is justified by _new observed contention_.

---

## Why this still converges fast enough

Because:

- Each `get()` loop iteration re-checks
    
- `in_flight` grows quickly under load
    
- Approvals are cheap to mint
    
- Multiple connect tasks run in parallel (up to `max_size`)
    

So the pool still ramps up ‚Äî just **smoothly**, not explosively.

---

## One-sentence intuition

> **Trickle scaling turns sudden demand spikes into controlled, step-by-step supply increases, instead of speculative over-creation.**

If you want, I can show a tiny timeline diagram (numbers over time) to make this visually obvious.