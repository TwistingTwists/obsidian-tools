---
tags:
  - half-baked
---
LLMs are search machines with vias towards correct answer. Drunk gambler ruin And LLMs Each task getting right - problem P Vote difficulty = ( 1-p) / p 

Assumption is that p > 0.5 

LLMs are better than monkeys. On a tiny task. (Low ambiguity) 

Practical implication: This model only applies when each step is more likely correct than incorrect. For LLMs, this means: Fine-grained steps must be well-defined enough that the model beats random chance Complex steps with  p < 0.5  break the model entirely