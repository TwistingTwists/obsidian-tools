
"smart_sources:Reading Papers.md": {"path":"Reading Papers.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1","at":1742188256276},"class_name":"SmartSource","outlinks":[{"title":"https://github.com/dair-ai/ML-Papers-of-the-Week","target":"https://github.com/dair-ai/ML-Papers-of-the-Week","line":6},{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":18},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":22},{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":57},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":61},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":76},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":80},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":88},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":92},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":97},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":102},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":107},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":112}],"metadata":{"tags":["research_papers"]},"blocks":{"#---frontmatter---":[1,4],"#":[6,16],"##MORA":[17,51],"##MORA#{1}":[18,25],"##MORA#Overview of Detection Transformers":[26,30],"##MORA#Overview of Detection Transformers#{1}":[28,30],"##MORA#Gemma - Google":[31,36],"##MORA#Gemma - Google#{1}":[32,36],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs":[37,40],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs#{1}":[39,40],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control":[41,51],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{1}":[43,45],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}":[46,46],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{3}":[47,47],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{4}":[48,51],"##Self Rewarding LLM":[52,142],"##Self Rewarding LLM#{1}":[54,60],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)":[61,75],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{1}":[63,63],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{2}":[64,65],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}":[66,70],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{4}":[71,73],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{5}":[74,75],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)":[76,79],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)#{1}":[78,79],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)":[80,83],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)#{1}":[82,83],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control":[84,87],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control#{1}":[86,87],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)":[88,91],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)#{1}":[90,91],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)":[92,96],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)#{1}":[94,96],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)":[97,101],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)#{1}":[100,101],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)":[102,106],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)#{1}":[104,106],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489":[107,111],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489#{1}":[108,111],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)":[112,115],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)#{1}":[114,115],"##Self Rewarding LLM#LLM Agents":[116,142],"##Self Rewarding LLM#LLM Agents#What":[118,133],"##Self Rewarding LLM#LLM Agents#What#{1}":[120,133],"##Self Rewarding LLM#LLM Agents#Failing":[134,142],"##Self Rewarding LLM#LLM Agents#Failing#{1}":[135,136],"##Self Rewarding LLM#LLM Agents#Failing#{2}":[137,138],"##Self Rewarding LLM#LLM Agents#Failing#{3}":[139,140],"##Self Rewarding LLM#LLM Agents#Failing#{4}":[141,142],"###":[143,143]},"last_import":{"mtime":1716978190964,"size":4304,"at":1742188256277,"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}},
"smart_sources:Reading Papers.md": {"path":"Reading Papers.md","last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}}},"last_read":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1","at":1742188603627},"class_name":"SmartSource","outlinks":[{"title":"https://github.com/dair-ai/ML-Papers-of-the-Week","target":"https://github.com/dair-ai/ML-Papers-of-the-Week","line":6},{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":18},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":22},{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":57},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":61},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":76},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":80},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":88},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":92},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":97},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":102},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":107},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":112}],"metadata":{"tags":["research_papers"]},"blocks":{"#---frontmatter---":[1,4],"#":[6,16],"##MORA":[17,51],"##MORA#{1}":[18,25],"##MORA#Overview of Detection Transformers":[26,30],"##MORA#Overview of Detection Transformers#{1}":[28,30],"##MORA#Gemma - Google":[31,36],"##MORA#Gemma - Google#{1}":[32,36],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs":[37,40],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs#{1}":[39,40],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control":[41,51],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{1}":[43,45],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}":[46,46],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{3}":[47,47],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{4}":[48,51],"##Self Rewarding LLM":[52,142],"##Self Rewarding LLM#{1}":[54,60],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)":[61,75],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{1}":[63,63],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{2}":[64,65],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}":[66,70],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{4}":[71,73],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{5}":[74,75],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)":[76,79],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)#{1}":[78,79],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)":[80,83],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)#{1}":[82,83],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control":[84,87],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control#{1}":[86,87],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)":[88,91],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)#{1}":[90,91],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)":[92,96],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)#{1}":[94,96],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)":[97,101],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)#{1}":[100,101],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)":[102,106],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)#{1}":[104,106],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489":[107,111],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489#{1}":[108,111],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)":[112,115],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)#{1}":[114,115],"##Self Rewarding LLM#LLM Agents":[116,142],"##Self Rewarding LLM#LLM Agents#What":[118,133],"##Self Rewarding LLM#LLM Agents#What#{1}":[120,133],"##Self Rewarding LLM#LLM Agents#Failing":[134,142],"##Self Rewarding LLM#LLM Agents#Failing#{1}":[135,136],"##Self Rewarding LLM#LLM Agents#Failing#{2}":[137,138],"##Self Rewarding LLM#LLM Agents#Failing#{3}":[139,140],"##Self Rewarding LLM#LLM Agents#Failing#{4}":[141,142],"###":[143,143]},"last_import":{"mtime":1716978190964,"size":4304,"at":0,"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}},"smart_blocks:Reading Papers.md##MORA": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea"}}},"text":null,"length":0,"last_read":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea","at":1742188601682},"key":"Reading Papers.md##MORA","lines":[17,51],"size":1107,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":2},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":6}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48"}}},"text":null,"length":0,"last_read":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48","at":1742188601839},"key":"Reading Papers.md##MORA#{1}","lines":[18,25],"size":284,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":1},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":5}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328"}}},"text":null,"length":0,"last_read":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328","at":1742188602026},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control","lines":[41,51],"size":425,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7"}}},"text":null,"length":0,"last_read":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7","at":1742188602194},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}","lines":[46,46],"size":212,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367"}}},"text":null,"length":0,"last_read":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367","at":1742188602387},"key":"Reading Papers.md##Self Rewarding LLM","lines":[52,142],"size":2948,"outlinks":[{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":6},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":10},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":25},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":29},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":37},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":41},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":46},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":51},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":56},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":61}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76"}}},"text":null,"length":0,"last_read":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76","at":1742188602573},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)","lines":[61,75],"size":741,"outlinks":[{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":1}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294"}}},"text":null,"length":0,"last_read":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294","at":1742188602738},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}","lines":[66,70],"size":389,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6"}}},"text":null,"length":0,"last_read":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6","at":1742188602926},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents","lines":[116,142],"size":864,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7"}}},"text":null,"length":0,"last_read":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7","at":1742188603128},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What","lines":[118,133],"size":314,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0"}}},"text":null,"length":0,"last_read":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0","at":1742188603303},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}","lines":[120,133],"size":303,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2"}}},"text":null,"length":0,"last_read":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2","at":1742188603464},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing","lines":[134,142],"size":531,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8"}}},"text":null,"length":0,"last_read":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8","at":1742188603629},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}","lines":[139,140],"size":211,"outlinks":[],"class_name":"SmartBlock"},

"smart_sources:Reading Papers.md": {"path":"Reading Papers.md","last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}}},"last_read":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1","at":1742188603627},"class_name":"SmartSource","outlinks":[{"title":"https://github.com/dair-ai/ML-Papers-of-the-Week","target":"https://github.com/dair-ai/ML-Papers-of-the-Week","line":6},{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":18},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":22},{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":57},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":61},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":76},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":80},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":88},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":92},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":97},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":102},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":107},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":112}],"metadata":{"tags":["research_papers"]},"blocks":{"#---frontmatter---":[1,4],"#":[6,16],"##MORA":[17,51],"##MORA#{1}":[18,25],"##MORA#Overview of Detection Transformers":[26,30],"##MORA#Overview of Detection Transformers#{1}":[28,30],"##MORA#Gemma - Google":[31,36],"##MORA#Gemma - Google#{1}":[32,36],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs":[37,40],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs#{1}":[39,40],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control":[41,51],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{1}":[43,45],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}":[46,46],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{3}":[47,47],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{4}":[48,51],"##Self Rewarding LLM":[52,142],"##Self Rewarding LLM#{1}":[54,60],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)":[61,75],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{1}":[63,63],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{2}":[64,65],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}":[66,70],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{4}":[71,73],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{5}":[74,75],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)":[76,79],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)#{1}":[78,79],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)":[80,83],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)#{1}":[82,83],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control":[84,87],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control#{1}":[86,87],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)":[88,91],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)#{1}":[90,91],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)":[92,96],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)#{1}":[94,96],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)":[97,101],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)#{1}":[100,101],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)":[102,106],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)#{1}":[104,106],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489":[107,111],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489#{1}":[108,111],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)":[112,115],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)#{1}":[114,115],"##Self Rewarding LLM#LLM Agents":[116,142],"##Self Rewarding LLM#LLM Agents#What":[118,133],"##Self Rewarding LLM#LLM Agents#What#{1}":[120,133],"##Self Rewarding LLM#LLM Agents#Failing":[134,142],"##Self Rewarding LLM#LLM Agents#Failing#{1}":[135,136],"##Self Rewarding LLM#LLM Agents#Failing#{2}":[137,138],"##Self Rewarding LLM#LLM Agents#Failing#{3}":[139,140],"##Self Rewarding LLM#LLM Agents#Failing#{4}":[141,142],"###":[143,143]},"last_import":{"mtime":1716978190964,"size":4304,"at":0,"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}},"smart_blocks:Reading Papers.md##MORA": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea"}}},"text":null,"length":0,"last_read":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea","at":1742188601682},"key":"Reading Papers.md##MORA","lines":[17,51],"size":1107,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":2},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":6}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48"}}},"text":null,"length":0,"last_read":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48","at":1742188601839},"key":"Reading Papers.md##MORA#{1}","lines":[18,25],"size":284,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":1},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":5}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328"}}},"text":null,"length":0,"last_read":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328","at":1742188602026},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control","lines":[41,51],"size":425,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7"}}},"text":null,"length":0,"last_read":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7","at":1742188602194},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}","lines":[46,46],"size":212,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367"}}},"text":null,"length":0,"last_read":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367","at":1742188602387},"key":"Reading Papers.md##Self Rewarding LLM","lines":[52,142],"size":2948,"outlinks":[{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":6},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":10},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":25},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":29},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":37},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":41},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":46},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":51},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":56},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":61}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76"}}},"text":null,"length":0,"last_read":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76","at":1742188602573},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)","lines":[61,75],"size":741,"outlinks":[{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":1}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294"}}},"text":null,"length":0,"last_read":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294","at":1742188602738},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}","lines":[66,70],"size":389,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6"}}},"text":null,"length":0,"last_read":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6","at":1742188602926},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents","lines":[116,142],"size":864,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7"}}},"text":null,"length":0,"last_read":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7","at":1742188603128},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What","lines":[118,133],"size":314,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0"}}},"text":null,"length":0,"last_read":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0","at":1742188603303},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}","lines":[120,133],"size":303,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2"}}},"text":null,"length":0,"last_read":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2","at":1742188603464},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing","lines":[134,142],"size":531,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8"}}},"text":null,"length":0,"last_read":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8","at":1742188603629},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}","lines":[139,140],"size":211,"outlinks":[],"class_name":"SmartBlock"},

"smart_sources:Reading Papers.md": {"path":"Reading Papers.md","last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}}},"last_read":{"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1","at":1742188603627},"class_name":"SmartSource","outlinks":[{"title":"https://github.com/dair-ai/ML-Papers-of-the-Week","target":"https://github.com/dair-ai/ML-Papers-of-the-Week","line":6},{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":18},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":22},{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":57},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":61},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":76},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":80},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":88},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":92},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":97},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":102},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":107},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":112}],"metadata":{"tags":["research_papers"]},"blocks":{"#---frontmatter---":[1,4],"#":[6,16],"##MORA":[17,51],"##MORA#{1}":[18,25],"##MORA#Overview of Detection Transformers":[26,30],"##MORA#Overview of Detection Transformers#{1}":[28,30],"##MORA#Gemma - Google":[31,36],"##MORA#Gemma - Google#{1}":[32,36],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs":[37,40],"##MORA#Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs#{1}":[39,40],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control":[41,51],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{1}":[43,45],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}":[46,46],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{3}":[47,47],"##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{4}":[48,51],"##Self Rewarding LLM":[52,142],"##Self Rewarding LLM#{1}":[54,60],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)":[61,75],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{1}":[63,63],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{2}":[64,65],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}":[66,70],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{4}":[71,73],"##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{5}":[74,75],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)":[76,79],"##Self Rewarding LLM#Reinforced Self-Training (ReST) for Language Modeling [Source](https://arxiv.org/abs/2308.08998)#{1}":[78,79],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)":[80,83],"##Self Rewarding LLM#Open-World Object Manipulation using Pre-trained Vision-Language Models [Source](https://arxiv.org/abs/2303.00905)#{1}":[82,83],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control":[84,87],"##Self Rewarding LLM#RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control#{1}":[86,87],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)":[88,91],"##Self Rewarding LLM#Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions [Source](https://arxiv.org/abs/2309.10150)#{1}":[90,91],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)":[92,96],"##Self Rewarding LLM#Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation [Source ](https://arxiv.org/abs/2211.02127)#{1}":[94,96],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)":[97,101],"##Self Rewarding LLM#Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems [Source](https://arxiv.org/abs/2206.07808)#{1}":[100,101],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)":[102,106],"##Self Rewarding LLM#Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models [Source](https://arxiv.org/abs/2206.04615)#{1}":[104,106],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489":[107,111],"##Self Rewarding LLM#\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations [Source](https://arxiv.org/abs/2109.13489#{1}":[108,111],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)":[112,115],"##Self Rewarding LLM#Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access [Source](https://arxiv.org/abs/2006.03533)#{1}":[114,115],"##Self Rewarding LLM#LLM Agents":[116,142],"##Self Rewarding LLM#LLM Agents#What":[118,133],"##Self Rewarding LLM#LLM Agents#What#{1}":[120,133],"##Self Rewarding LLM#LLM Agents#Failing":[134,142],"##Self Rewarding LLM#LLM Agents#Failing#{1}":[135,136],"##Self Rewarding LLM#LLM Agents#Failing#{2}":[137,138],"##Self Rewarding LLM#LLM Agents#Failing#{3}":[139,140],"##Self Rewarding LLM#LLM Agents#Failing#{4}":[141,142],"###":[143,143]},"last_import":{"mtime":1716978190964,"size":4304,"at":0,"hash":"2790e84d770eed1945c2c945558a13242008bde05d73d872b88272436b0462e1"}},"smart_blocks:Reading Papers.md##MORA": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea"}}},"text":null,"length":0,"last_read":{"hash":"2d5d1d1d429298a16a769c04f40a471947fd9489e44baa5888119dff8bc26eea","at":1742188601682},"key":"Reading Papers.md##MORA","lines":[17,51],"size":1107,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":2},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":6}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48"}}},"text":null,"length":0,"last_read":{"hash":"82183dc023eedc9a4fa9dca59a8890a57becc92c0b58114e539829a8d7f97c48","at":1742188601839},"key":"Reading Papers.md##MORA#{1}","lines":[18,25],"size":284,"outlinks":[{"title":"Paper","target":"https://arxiv.org/abs/2403.13248","line":1},{"title":"Paper","target":"https://huggingface.co/papers/2403.13372?utm_source=digest-papers&utm_medium=email&utm_campaign=2024-03-21","line":5}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328"}}},"text":null,"length":0,"last_read":{"hash":"82dafa950913ba3f9ed2154f0af5b84f8397e4ed273491051a6954396b3a0328","at":1742188602026},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control","lines":[41,51],"size":425,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7"}}},"text":null,"length":0,"last_read":{"hash":"32a72363bee084b849493e463786c7e75c4098316138d77fba3f4ce256a056d7","at":1742188602194},"key":"Reading Papers.md##MORA#Learning to Learn Faster from Human Feedback with Language Model Predictive Control#{2}","lines":[46,46],"size":212,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367"}}},"text":null,"length":0,"last_read":{"hash":"d870809caa9d926181c086d2fc1c2a74972d0f402b8c38adf7bdf7a38adfc367","at":1742188602387},"key":"Reading Papers.md##Self Rewarding LLM","lines":[52,142],"size":2948,"outlinks":[{"title":"Good Summary ","target":"https://www.youtube.com/watch?v=PeSLWTZ1Yg8&t=2s","line":6},{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":10},{"title":"Source","target":"https://arxiv.org/abs/2308.08998","line":25},{"title":"Source","target":"https://arxiv.org/abs/2303.00905","line":29},{"title":"Source","target":"https://arxiv.org/abs/2309.10150","line":37},{"title":"Source ","target":"https://arxiv.org/abs/2211.02127","line":41},{"title":"Source","target":"https://arxiv.org/abs/2206.07808","line":46},{"title":"Source","target":"https://arxiv.org/abs/2206.04615","line":51},{"title":"Source","target":"https://arxiv.org/abs/2109.13489\n","line":56},{"title":"Source","target":"https://arxiv.org/abs/2006.03533","line":61}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76"}}},"text":null,"length":0,"last_read":{"hash":"4fcfe8ddce7aa5a76dc35c3bf5ac095425d0b2b9032e5f85b9d78abe482bde76","at":1742188602573},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)","lines":[61,75],"size":741,"outlinks":[{"title":"Source","target":"https://arxiv.org/pdf/2212.09689.pdf","line":1}],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294"}}},"text":null,"length":0,"last_read":{"hash":"3bea8752181d7fda33e238e7ad616f2155a48f9dc502d00a29341b6acfb5c294","at":1742188602738},"key":"Reading Papers.md##Self Rewarding LLM#Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor [Source](https://arxiv.org/pdf/2212.09689.pdf)#{3}","lines":[66,70],"size":389,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6"}}},"text":null,"length":0,"last_read":{"hash":"72878c427c5ba29b2f26b878c02474b9c67443475c229c6127ca6b414791f1f6","at":1742188602926},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents","lines":[116,142],"size":864,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7"}}},"text":null,"length":0,"last_read":{"hash":"e5ca3ff8344a00deb8059eb8160803c93d8377e5746665d579a228f2c20df0c7","at":1742188603128},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What","lines":[118,133],"size":314,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0"}}},"text":null,"length":0,"last_read":{"hash":"a8cd06dfbccac1aa9fbe87f2b466cfc718790efd800b0ad60247e85758ffffd0","at":1742188603303},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#What#{1}","lines":[120,133],"size":303,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2"}}},"text":null,"length":0,"last_read":{"hash":"b63d1947a5347937a18d72bd3d05d6bff0cd191bbc14ffd0884e3cd55caa00d2","at":1742188603464},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing","lines":[134,142],"size":531,"outlinks":[],"class_name":"SmartBlock"},
"smart_blocks:Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}": {"path":null,"last_embed":{"hash":null},"embeddings":{"TaylorAI/bge-micro-v2":{"last_embed":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8"}}},"text":null,"length":0,"last_read":{"hash":"9687dd72a67c7dfe937523602919059c6f7533675b2625ade76516a07e9503d8","at":1742188603629},"key":"Reading Papers.md##Self Rewarding LLM#LLM Agents#Failing#{3}","lines":[139,140],"size":211,"outlinks":[],"class_name":"SmartBlock"},
