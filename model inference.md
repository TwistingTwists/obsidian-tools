---
tags:
  - gpu
---
- [Inside vLLM: Anatomy of a High-Throughput LLM Inference System - Aleksa GordiÄ‡](https://www.aleksagordic.com/blog/vllm)  
- [GitHub - GeeeekExplorer/nano-vllm: Nano vLLM](https://github.com/GeeeekExplorer/nano-vllm)  
- [Life of an inference request (vLLM V1): How LLMs are served efficiently at scale](https://www.ubicloud.com/blog/life-of-an-inference-request-vllm-v1)

